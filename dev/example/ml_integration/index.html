<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>machine learning · Reversi.jl</title><meta name="title" content="machine learning · Reversi.jl"/><meta property="og:title" content="machine learning · Reversi.jl"/><meta property="twitter:title" content="machine learning · Reversi.jl"/><meta name="description" content="Documentation for Reversi.jl."/><meta property="og:description" content="Documentation for Reversi.jl."/><meta property="twitter:description" content="Documentation for Reversi.jl."/><meta property="og:url" content="https://codes.sota-shimozono.com/Reversi.jl/stable/example/ml_integration/"/><meta property="twitter:url" content="https://codes.sota-shimozono.com/Reversi.jl/stable/example/ml_integration/"/><link rel="canonical" href="https://codes.sota-shimozono.com/Reversi.jl/stable/example/ml_integration/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Reversi.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Reversi.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../demo/">quickstart</a></li><li><a class="tocitem" href="../play/">play</a></li><li class="is-active"><a class="tocitem" href>machine learning</a><ul class="internal"><li><a class="tocitem" href="#Example-1:-Simple-Heuristic-Player"><span>Example 1: Simple Heuristic Player</span></a></li><li><a class="tocitem" href="#Example-2:-Player-that-tracks-game-state-for-ML-training"><span>Example 2: Player that tracks game state for ML training</span></a></li><li><a class="tocitem" href="#Example-3:-Mock-Neural-Network-Player"><span>Example 3: Mock Neural Network Player</span></a></li></ul></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../../api/struct/">struct</a></li><li><a class="tocitem" href="../../api/rules/">rules</a></li><li><a class="tocitem" href="../../api/player/">player</a></li><li><a class="tocitem" href="../../api/game/">game</a></li><li><a class="tocitem" href="../../api/data/">data</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>machine learning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>machine learning</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sotashimozono/Reversi.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sotashimozono/Reversi.jl/blob/main/examples/ml_integration.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Example:-Creating-a-Custom-AI-Player-for-Machine-Learning-Integration"><a class="docs-heading-anchor" href="#Example:-Creating-a-Custom-AI-Player-for-Machine-Learning-Integration">Example: Creating a Custom AI Player for Machine Learning Integration</a><a id="Example:-Creating-a-Custom-AI-Player-for-Machine-Learning-Integration-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Creating-a-Custom-AI-Player-for-Machine-Learning-Integration" title="Permalink"></a></h1><p>This example demonstrates how to create custom players that can integrate with machine learning frameworks like Flux.jl or reinforcement learning libraries.</p><pre><code class="language-julia hljs">using Reversi
using Reversi: BLACK, WHITE, EMPTY, count_pieces, is_game_over, opponent</code></pre><h2 id="Example-1:-Simple-Heuristic-Player"><a class="docs-heading-anchor" href="#Example-1:-Simple-Heuristic-Player">Example 1: Simple Heuristic Player</a><a id="Example-1:-Simple-Heuristic-Player-1"></a><a class="docs-heading-anchor-permalink" href="#Example-1:-Simple-Heuristic-Player" title="Permalink"></a></h2><p>This player uses a position-based heuristic Corner squares are valuable, edges are decent, next to corners are bad</p><pre><code class="language-julia hljs">struct HeuristicPlayer &lt;: Player
    weights::Matrix{Float64}

    function HeuristicPlayer()
        weights = [
            100 -20 10 5 5 10 -20 100;
            -20 -50 -2 -2 -2 -2 -50 -20;
            10 -2 5 3 3 5 -2 10;
            5 -2 3 1 1 3 -2 5;
            5 -2 3 1 1 3 -2 5;
            10 -2 5 3 3 5 -2 10;
            -20 -50 -2 -2 -2 -2 -50 -20;
            100 -20 10 5 5 10 -20 100
        ]
        new(weights)
    end
end

function Reversi.get_move(player::HeuristicPlayer, game::ReversiGame)
    moves = valid_moves(game)

    if isempty(moves)
        return nothing
    end

    best_move = moves[1]
    best_score = player.weights[best_move.row, best_move.col]

    for move in moves[2:end]
        score = player.weights[move.row, move.col]
        if score &gt; best_score
            best_score = score
            best_move = move
        end
    end

    return best_move
end</code></pre><h2 id="Example-2:-Player-that-tracks-game-state-for-ML-training"><a class="docs-heading-anchor" href="#Example-2:-Player-that-tracks-game-state-for-ML-training">Example 2: Player that tracks game state for ML training</a><a id="Example-2:-Player-that-tracks-game-state-for-ML-training-1"></a><a class="docs-heading-anchor-permalink" href="#Example-2:-Player-that-tracks-game-state-for-ML-training" title="Permalink"></a></h2><p>Board state is represented as a plain <code>Matrix{Int}</code> where <code>0</code> = empty, <code>1</code> = BLACK, <code>2</code> = WHITE — no external dependencies required.</p><pre><code class="language-julia hljs">mutable struct TrainingPlayer &lt;: Player
    move_history::Vector{Tuple{Matrix{Int},Position}}

    TrainingPlayer() = new(Tuple{Matrix{Int},Position}[])
end

&quot;&quot;&quot;
    board_to_matrix(game::ReversiGame) -&gt; Matrix{Int}

Convert the bitboard state of `game` into an 8×8 `Matrix{Int}` where
`EMPTY == 0`, `BLACK == 1`, and `WHITE == 2`.
&quot;&quot;&quot;
function board_to_matrix(game::ReversiGame)
    mat = zeros(Int, 8, 8)
    for row in 1:8, col in 1:8
        mat[row, col] = get_piece(game, row, col)
    end
    return mat
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Main.board_to_matrix</code></pre><p>Record the board state as a plain matrix Make a random move (in practice, this would use your ML model) Store the state-action pair for training</p><pre><code class="language-julia hljs">function Reversi.get_move(player::TrainingPlayer, game::ReversiGame)
    moves = valid_moves(game)

    if isempty(moves)
        return nothing
    end

    board_copy = board_to_matrix(game)
    move = rand(moves)
    push!(player.move_history, (board_copy, move))

    return move
end</code></pre><h2 id="Example-3:-Mock-Neural-Network-Player"><a class="docs-heading-anchor" href="#Example-3:-Mock-Neural-Network-Player">Example 3: Mock Neural Network Player</a><a id="Example-3:-Mock-Neural-Network-Player-1"></a><a class="docs-heading-anchor-permalink" href="#Example-3:-Mock-Neural-Network-Player" title="Permalink"></a></h2><p>This shows the structure for integrating with a real neural network In practice, this would be your trained model from Flux, TensorFlow, etc.</p><pre><code class="language-julia hljs">struct NeuralNetPlayer &lt;: Player
    model
end</code></pre><p>Convert board to neural network input In practice: input = preprocess<em>board(game.board, game.current</em>player) For demo, we&#39;ll just use random</p><p>Get policy from neural network In practice: policy = player.model(input)</p><p>Select move based on policy In practice: return select<em>move</em>from_policy(policy, moves)</p><pre><code class="language-julia hljs">function Reversi.get_move(player::NeuralNetPlayer, game::ReversiGame)
    moves = valid_moves(game)

    if isempty(moves)
        return nothing
    end

    return rand(moves)
end</code></pre><p>Example 4: Minimax Player (game tree search)</p><pre><code class="language-julia hljs">struct MinimaxPlayer &lt;: Player
    depth::Int
    MinimaxPlayer(depth=3) = new(depth)
end</code></pre><p>Simple evaluation: piece count difference</p><pre><code class="language-julia hljs">function evaluate_board(game::ReversiGame, player_color::Int)
    black, white = count_pieces(game)
    return player_color == BLACK ? (black - white) : (white - black)
end

function minimax(
    game::ReversiGame,
    depth::Int,
    maximizing::Bool,
    alpha::Float64,
    beta::Float64,
    player_color::Int,
)
    if depth == 0 || is_game_over(game)
        return evaluate_board(game, player_color)
    end

    moves = valid_moves(game)

    if isempty(moves)
        temp_game = deepcopy(game)
        temp_game.current_player = opponent(temp_game.current_player)
        temp_game.pass_count += 1
        return minimax(temp_game, depth - 1, !maximizing, alpha, beta, player_color)
    end

    if maximizing
        max_eval = -Inf
        for move in moves
            temp_game = deepcopy(game)
            make_move!(temp_game, move.row, move.col)
            eval = minimax(temp_game, depth - 1, false, alpha, beta, player_color)
            max_eval = max(max_eval, eval)
            alpha = max(alpha, eval)
            if beta &lt;= alpha
                break
            end
        end
        return max_eval
    else
        min_eval = Inf
        for move in moves
            temp_game = deepcopy(game)
            make_move!(temp_game, move.row, move.col)
            eval = minimax(temp_game, depth - 1, true, alpha, beta, player_color)
            min_eval = min(min_eval, eval)
            beta = min(beta, eval)
            if beta &lt;= alpha
                break
            end
        end
        return min_eval
    end
end

function Reversi.get_move(player::MinimaxPlayer, game::ReversiGame)
    moves = valid_moves(game)

    if isempty(moves)
        return nothing
    end

    best_move = moves[1]
    best_score = -Inf

    for move in moves
        temp_game = deepcopy(game)
        make_move!(temp_game, move.row, move.col)
        score = minimax(temp_game, player.depth - 1, false, -Inf, Inf, game.current_player)

        if score &gt; best_score
            best_score = score
            best_move = move
        end
    end

    return best_move
end

println(&quot;=&quot;^60)
println(&quot;Machine Learning Integration Examples&quot;)
println(&quot;=&quot;^60)
println()

println(&quot;Test 1: Heuristic Player vs Random&quot;)
println(&quot;-&quot;^60)
winner = play_game(HeuristicPlayer(), RandomPlayer(); verbose=false)
winner_str =
    winner == BLACK ? &quot;Heuristic (Black)&quot; : (winner == WHITE ? &quot;Random (White)&quot; : &quot;Draw&quot;)
println(&quot;Winner: $winner_str&quot;)
println()

println(&quot;Test 2: Training Player (collecting data)&quot;)
println(&quot;-&quot;^60)
training_player = TrainingPlayer()
winner = play_game(training_player, RandomPlayer(); verbose=false)
println(&quot;Collected $(length(training_player.move_history)) state-action pairs for training&quot;)
println()

println(&quot;Test 3: Minimax Player vs Random (depth=2)&quot;)
println(&quot;-&quot;^60)
winner = play_game(MinimaxPlayer(2), RandomPlayer(); verbose=false)
winner_str =
    winner == BLACK ? &quot;Minimax (Black)&quot; : (winner == WHITE ? &quot;Random (White)&quot; : &quot;Draw&quot;)
println(&quot;Winner: $winner_str&quot;)
println()

println(&quot;=&quot;^60)
println(&quot;Integration Examples Complete!&quot;)
println()
println(&quot;Key Takeaways:&quot;)
println(&quot;  1. Abstract Player type enables easy custom implementations&quot;)
println(&quot;  2. Game state can be copied for tree search (Minimax, MCTS)&quot;)
println(&quot;  3. State-action pairs can be collected for supervised learning&quot;)
println(&quot;  4. Neural network policies can be integrated seamlessly&quot;)
println(&quot;=&quot;^60)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">============================================================
Machine Learning Integration Examples
============================================================

Test 1: Heuristic Player vs Random
------------------------------------------------------------
Winner: Heuristic (Black)

Test 2: Training Player (collecting data)
------------------------------------------------------------
Collected 30 state-action pairs for training

Test 3: Minimax Player vs Random (depth=2)
------------------------------------------------------------
Winner: Minimax (Black)

============================================================
Integration Examples Complete!

Key Takeaways:
  1. Abstract Player type enables easy custom implementations
  2. Game state can be copied for tree search (Minimax, MCTS)
  3. State-action pairs can be collected for supervised learning
  4. Neural network policies can be integrated seamlessly
============================================================</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../play/">« play</a><a class="docs-footer-nextpage" href="../../api/struct/">struct »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 20 February 2026 07:53">Friday 20 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
